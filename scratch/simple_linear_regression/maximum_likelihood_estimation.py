# Why choose least squares? One justification invokes maximum likelihookd estimation. 
# Imagine that we have a sample of data v1, ..., vn that comes from a distribution 
# that depends on some unknown parameter θ (theta):

# P(v1, ..., vn|θ)

# If we didn't know θ, we could turn around and think of this quantity as the 
# likelihood of θ given the sample:

# L(θ|v1, ..., vn)

# Under this approach, the most likely θ is the value that maximizes this likelihook 
# function -- that is, the value that makes the observed data the most probable. 
# In the case of a continuous distribution, in which we have a probability 
# distribution function rather than a probability mass function, we can do the 
# same thing.
